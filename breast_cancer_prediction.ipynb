{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c750dc",
   "metadata": {},
   "source": [
    "# Breast Cancer Prediction using Machine Learning\n",
    "\n",
    "This comprehensive machine learning project focuses on predicting breast cancer diagnosis using the Wisconsin Breast Cancer dataset. We'll implement multiple algorithms, perform thorough feature analysis, and compare different approaches to build an optimal classification model.\n",
    "\n",
    "## Project Overview\n",
    "- **Dataset**: Wisconsin Breast Cancer Dataset\n",
    "- **Objective**: Classify tumors as malignant or benign\n",
    "- **Approach**: Multiple ML algorithms with comprehensive evaluation\n",
    "- **Tools**: Python, scikit-learn, pandas, matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac370dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# ML Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac18848",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration\n",
    "\n",
    "Let's start by loading the breast cancer dataset and examining its structure, checking for missing values, and understanding the distribution of target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b92849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "# Create a complete dataframe\n",
    "df = X.copy()\n",
    "df['target'] = y\n",
    "df['diagnosis'] = df['target'].map({0: 'malignant', 1: 'benign'})\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Number of features: {len(X.columns)}\")\n",
    "print(f\"Target classes: {data.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0545132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found!\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(\"Target Distribution:\")\n",
    "target_counts = df['diagnosis'].value_counts()\n",
    "print(target_counts)\n",
    "print(f\"\\nPercentages:\")\n",
    "print(target_counts / len(df) * 100)\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "target_counts.plot(kind='bar', color=['coral', 'lightblue'])\n",
    "plt.title('Distribution of Diagnosis')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', colors=['coral', 'lightblue'])\n",
    "plt.title('Diagnosis Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0313d",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis and Visualization\n",
    "\n",
    "Now let's create comprehensive visualizations to understand feature relationships and identify patterns between malignant and benign cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categories analysis\n",
    "feature_categories = {\n",
    "    'mean': [col for col in X.columns if 'mean' in col],\n",
    "    'se': [col for col in X.columns if 'se' in col],\n",
    "    'worst': [col for col in X.columns if 'worst' in col]\n",
    "}\n",
    "\n",
    "print(\"Feature Categories:\")\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"{category.upper()}: {len(features)} features\")\n",
    "    print(features[:3], \"...\" if len(features) > 3 else \"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(20, 16))\n",
    "correlation_matrix = X.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54eebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for key features\n",
    "key_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    for diagnosis in df['diagnosis'].unique():\n",
    "        subset = df[df['diagnosis'] == diagnosis][feature]\n",
    "        axes[i].hist(subset, alpha=0.7, label=diagnosis, bins=20)\n",
    "    \n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for comparison between malignant and benign\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    sns.boxplot(data=df, x='diagnosis', y=feature, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Diagnosis')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4884d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix for selected features\n",
    "selected_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']\n",
    "scatter_df = df[selected_features + ['diagnosis']].copy()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "pd.plotting.scatter_matrix(scatter_df[selected_features], \n",
    "                          c=df['target'], \n",
    "                          figsize=(15, 12), \n",
    "                          alpha=0.7,\n",
    "                          diagonal='hist')\n",
    "plt.suptitle('Scatter Plot Matrix of Key Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44313c19",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Let's handle data cleaning, feature scaling, and create engineered features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593621ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X_processed = X.copy()\n",
    "y_processed = y.copy()\n",
    "\n",
    "print(\"Original feature shape:\", X_processed.shape)\n",
    "print(\"Target shape:\", y_processed.shape)\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(X_processed.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering - create ratio features\n",
    "X_processed['radius_texture_ratio'] = X_processed['mean radius'] / X_processed['mean texture']\n",
    "X_processed['area_perimeter_ratio'] = X_processed['mean area'] / X_processed['mean perimeter']\n",
    "X_processed['compactness_smoothness_ratio'] = X_processed['mean compactness'] / X_processed['mean smoothness']\n",
    "\n",
    "# Create polynomial features for key variables\n",
    "X_processed['radius_squared'] = X_processed['mean radius'] ** 2\n",
    "X_processed['area_sqrt'] = np.sqrt(X_processed['mean area'])\n",
    "\n",
    "print(\"Feature engineering completed.\")\n",
    "print(\"New feature shape:\", X_processed.shape)\n",
    "print(\"New features added:\", ['radius_texture_ratio', 'area_perimeter_ratio', 'compactness_smoothness_ratio', 'radius_squared', 'area_sqrt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ca4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y_processed)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Training target distribution:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "# StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "X_train_scaled = scaler_standard.fit_transform(X_train)\n",
    "X_test_scaled = scaler_standard.transform(X_test)\n",
    "\n",
    "# MinMaxScaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
    "X_test_minmax = scaler_minmax.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed using StandardScaler and MinMaxScaler\")\n",
    "print(\"Scaled training set shape:\", X_train_scaled_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d9ad4",
   "metadata": {},
   "source": [
    "## 4. Feature Selection and Analysis\n",
    "\n",
    "Let's implement multiple feature selection techniques to identify the most predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa226d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "feature_target_corr = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'correlation': [np.corrcoef(X_train[col], y_train)[0,1] for col in X_train.columns]\n",
    "})\n",
    "feature_target_corr['abs_correlation'] = abs(feature_target_corr['correlation'])\n",
    "feature_target_corr = feature_target_corr.sort_values('abs_correlation', ascending=False)\n",
    "\n",
    "print(\"Top 15 features by correlation with target:\")\n",
    "print(feature_target_corr.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517dea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate feature selection\n",
    "selector_univariate = SelectKBest(score_func=f_classif, k=20)\n",
    "X_train_univariate = selector_univariate.fit_transform(X_train_scaled, y_train)\n",
    "X_test_univariate = selector_univariate.transform(X_test_scaled)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features_univariate = X_train.columns[selector_univariate.get_support()]\n",
    "print(\"Features selected by univariate selection:\")\n",
    "print(list(selected_features_univariate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b11cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination with Random Forest\n",
    "rf_estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector_rfe = RFE(estimator=rf_estimator, n_features_to_select=15)\n",
    "X_train_rfe = selector_rfe.fit_transform(X_train_scaled, y_train)\n",
    "X_test_rfe = selector_rfe.transform(X_test_scaled)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features_rfe = X_train.columns[selector_rfe.get_support()]\n",
    "print(\"Features selected by RFE:\")\n",
    "print(list(selected_features_rfe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_importance = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_importance.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_importance.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 features by Random Forest importance:\")\n",
    "print(feature_importance_df.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754ed1d",
   "metadata": {},
   "source": [
    "## 5. Model Implementation and Training\n",
    "\n",
    "Now let's implement and train multiple machine learning algorithms to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Neural Network': MLPClassifier(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "print(\"Models initialized:\")\n",
    "for name in models.keys():\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and store results\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78954947",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Evaluation\n",
    "\n",
    "Let's evaluate all models using various metrics and visualization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b79e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(model_results.keys()),\n",
    "    'Accuracy': [results['accuracy'] for results in model_results.values()],\n",
    "    'Precision': [results['precision'] for results in model_results.values()],\n",
    "    'Recall': [results['recall'] for results in model_results.values()],\n",
    "    'F1-Score': [results['f1_score'] for results in model_results.values()],\n",
    "    'CV Mean': [results['cv_mean'] for results in model_results.values()],\n",
    "    'CV Std': [results['cv_std'] for results in model_results.values()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe77196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0,0].bar(comparison_df['Model'], comparison_df['Accuracy'], color='skyblue')\n",
    "axes[0,0].set_title('Model Accuracy Comparison')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Precision comparison\n",
    "axes[0,1].bar(comparison_df['Model'], comparison_df['Precision'], color='lightgreen')\n",
    "axes[0,1].set_title('Model Precision Comparison')\n",
    "axes[0,1].set_ylabel('Precision')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Recall comparison\n",
    "axes[1,0].bar(comparison_df['Model'], comparison_df['Recall'], color='salmon')\n",
    "axes[1,0].set_title('Model Recall Comparison')\n",
    "axes[1,0].set_ylabel('Recall')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[1,1].bar(comparison_df['Model'], comparison_df['F1-Score'], color='gold')\n",
    "axes[1,1].set_title('Model F1-Score Comparison')\n",
    "axes[1,1].set_ylabel('F1-Score')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d769ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for top 3 models\n",
    "top_3_models = comparison_df.head(3)['Model'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, model_name in enumerate(top_3_models):\n",
    "    y_pred = model_results[model_name]['y_pred']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "    axes[i].set_title(f'Confusion Matrix - {model_name}')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, results in model_results.items():\n",
    "    if results['y_pred_proba'] is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'])\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6059b4",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Optimization\n",
    "\n",
    "Let's optimize hyperparameters for the best performing models to improve their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best performing model for optimization\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Current accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea880f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for Random Forest\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    grid_search_rf = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid_rf,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Optimizing Random Forest hyperparameters...\")\n",
    "    grid_search_rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"Best parameters:\", grid_search_rf.best_params_)\n",
    "    print(\"Best cross-validation score:\", grid_search_rf.best_score_)\n",
    "    \n",
    "    # Evaluate optimized model\n",
    "    best_rf = grid_search_rf.best_estimator_\n",
    "    y_pred_optimized = best_rf.predict(X_test_scaled)\n",
    "    accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
    "    print(f\"Optimized model accuracy: {accuracy_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73999a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for faster optimization\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    SVC(random_state=42, probability=True),\n",
    "    param_grid_svm,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Optimizing SVM hyperparameters...\")\n",
    "random_search_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search_svm.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search_svm.best_score_)\n",
    "\n",
    "# Evaluate optimized SVM\n",
    "best_svm = random_search_svm.best_estimator_\n",
    "y_pred_svm_optimized = best_svm.predict(X_test_scaled)\n",
    "accuracy_svm_optimized = accuracy_score(y_test, y_pred_svm_optimized)\n",
    "print(f\"Optimized SVM accuracy: {accuracy_svm_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2922aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(\n",
    "    LogisticRegression(random_state=42, max_iter=1000),\n",
    "    param_grid_lr,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Optimizing Logistic Regression hyperparameters...\")\n",
    "grid_search_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_lr.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search_lr.best_score_)\n",
    "\n",
    "# Evaluate optimized Logistic Regression\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "y_pred_lr_optimized = best_lr.predict(X_test_scaled)\n",
    "accuracy_lr_optimized = accuracy_score(y_test, y_pred_lr_optimized)\n",
    "print(f\"Optimized Logistic Regression accuracy: {accuracy_lr_optimized:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83a36b",
   "metadata": {},
   "source": [
    "## 8. Final Model Selection and Performance Analysis\n",
    "\n",
    "Let's select the best model and provide detailed analysis with clinical relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfc242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimized models\n",
    "optimized_results = {\n",
    "    'Random Forest (Optimized)': accuracy_optimized if 'accuracy_optimized' in locals() else 0,\n",
    "    'SVM (Optimized)': accuracy_svm_optimized,\n",
    "    'Logistic Regression (Optimized)': accuracy_lr_optimized\n",
    "}\n",
    "\n",
    "print(\"Optimized Model Performance:\")\n",
    "for model, accuracy in optimized_results.items():\n",
    "    print(f\"{model}: {accuracy:.4f}\")\n",
    "\n",
    "# Select the best optimized model\n",
    "best_optimized_model_name = max(optimized_results, key=optimized_results.get)\n",
    "best_accuracy = optimized_results[best_optimized_model_name]\n",
    "\n",
    "print(f\"\\nBest optimized model: {best_optimized_model_name}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation\n",
    "if 'SVM' in best_optimized_model_name:\n",
    "    final_model = best_svm\n",
    "elif 'Logistic' in best_optimized_model_name:\n",
    "    final_model = best_lr\n",
    "else:\n",
    "    final_model = best_rf\n",
    "\n",
    "# Final predictions\n",
    "y_pred_final = final_model.predict(X_test_scaled)\n",
    "y_pred_proba_final = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Comprehensive evaluation metrics\n",
    "print(\"Final Model Performance Report:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba_final):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c97962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for final model\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    # Tree-based model\n",
    "    feature_importance_final = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': final_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance_final.head(10))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_10_features = feature_importance_final.head(10)\n",
    "    plt.barh(range(len(top_10_features)), top_10_features['importance'])\n",
    "    plt.yticks(range(len(top_10_features)), top_10_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Feature Importances (Final Model)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif hasattr(final_model, 'coef_'):\n",
    "    # Linear model\n",
    "    feature_importance_final = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'coefficient': final_model.coef_[0],\n",
    "        'abs_coefficient': np.abs(final_model.coef_[0])\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features (by coefficient magnitude):\")\n",
    "    print(feature_importance_final.head(10))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_10_features = feature_importance_final.head(10)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_10_features['coefficient']]\n",
    "    plt.barh(range(len(top_10_features)), top_10_features['coefficient'], color=colors)\n",
    "    plt.yticks(range(len(top_10_features)), top_10_features['feature'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.title('Top 10 Feature Coefficients (Final Model)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe67f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final confusion matrix and ROC curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_final = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title(f'Final Model Confusion Matrix\\n{best_optimized_model_name}')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr_final, tpr_final, _ = roc_curve(y_test, y_pred_proba_final)\n",
    "auc_final = auc(fpr_final, tpr_final)\n",
    "axes[1].plot(fpr_final, tpr_final, color='blue', label=f'ROC Curve (AUC = {auc_final:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('Final Model ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aab6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clinical interpretation and insights\n",
    "print(\"CLINICAL INTERPRETATION AND INSIGHTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate clinical metrics\n",
    "tn, fp, fn, tp = cm_final.ravel()\n",
    "sensitivity = tp / (tp + fn)  # True Positive Rate\n",
    "specificity = tn / (tn + fp)  # True Negative Rate\n",
    "ppv = tp / (tp + fp)  # Positive Predictive Value\n",
    "npv = tn / (tn + fn)  # Negative Predictive Value\n",
    "\n",
    "print(f\"Clinical Performance Metrics:\")\n",
    "print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
    "print(f\"  - Ability to correctly identify malignant cases\")\n",
    "print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"  - Ability to correctly identify benign cases\")\n",
    "print(f\"Positive Predictive Value: {ppv:.4f}\")\n",
    "print(f\"  - Probability that a positive test indicates cancer\")\n",
    "print(f\"Negative Predictive Value: {npv:.4f}\")\n",
    "print(f\"  - Probability that a negative test indicates no cancer\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"True Positives (Correctly identified malignant): {tp}\")\n",
    "print(f\"True Negatives (Correctly identified benign): {tn}\")\n",
    "print(f\"False Positives (Benign classified as malignant): {fp}\")\n",
    "print(f\"False Negatives (Malignant classified as benign): {fn}\")\n",
    "\n",
    "print(f\"\\nClinical Significance:\")\n",
    "if fn > 0:\n",
    "    print(f\"⚠️  {fn} malignant cases were misclassified as benign (False Negatives)\")\n",
    "    print(\"   This is clinically concerning as it could delay treatment\")\n",
    "if fp > 0:\n",
    "    print(f\"⚠️  {fp} benign cases were misclassified as malignant (False Positives)\")\n",
    "    print(\"   This could lead to unnecessary anxiety and additional testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83f2dd",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Project Summary\n",
    "This comprehensive breast cancer prediction project successfully implemented and compared multiple machine learning algorithms to classify breast tumors as malignant or benign using the Wisconsin Breast Cancer dataset.\n",
    "\n",
    "### Key Findings:\n",
    "1. **Dataset**: 569 samples with 30 features, well-balanced between malignant and benign cases\n",
    "2. **Best Model**: The optimized model achieved high accuracy with excellent clinical performance\n",
    "3. **Important Features**: Radius, perimeter, area, and texture measurements were most predictive\n",
    "4. **Clinical Relevance**: The model shows strong potential for supporting medical diagnosis\n",
    "\n",
    "### Model Performance:\n",
    "- **High Sensitivity**: Excellent at detecting malignant cases\n",
    "- **High Specificity**: Effective at identifying benign cases  \n",
    "- **Low False Negatives**: Minimizes risk of missing cancer cases\n",
    "- **Balanced Performance**: Good balance between precision and recall\n",
    "\n",
    "### Clinical Applications:\n",
    "- **Diagnostic Support**: Can assist radiologists in tumor classification\n",
    "- **Screening Enhancement**: Potential for improving screening programs\n",
    "- **Risk Assessment**: Valuable for patient risk stratification\n",
    "- **Quality Assurance**: Could serve as a second opinion system\n",
    "\n",
    "### Future Improvements:\n",
    "1. **Data Augmentation**: Incorporate more diverse datasets\n",
    "2. **Deep Learning**: Explore advanced neural network architectures\n",
    "3. **Ensemble Methods**: Combine multiple models for better performance\n",
    "4. **Real-world Validation**: Test on external datasets from different institutions\n",
    "5. **Feature Engineering**: Develop domain-specific engineered features\n",
    "\n",
    "### Disclaimer:\n",
    "This model is developed for research and educational purposes. Any clinical application would require extensive validation, regulatory approval, and should always be used in conjunction with professional medical judgment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29796396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and results\n",
    "import joblib\n",
    "\n",
    "# Save the final model\n",
    "model_filename = 'breast_cancer_final_model.pkl'\n",
    "scaler_filename = 'breast_cancer_scaler.pkl'\n",
    "\n",
    "joblib.dump(final_model, model_filename)\n",
    "joblib.dump(scaler_standard, scaler_filename)\n",
    "\n",
    "print(f\"Final model saved as: {model_filename}\")\n",
    "print(f\"Scaler saved as: {scaler_filename}\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'best_model': best_optimized_model_name,\n",
    "    'accuracy': accuracy_score(y_test, y_pred_final),\n",
    "    'precision': precision_score(y_test, y_pred_final),\n",
    "    'recall': recall_score(y_test, y_pred_final),\n",
    "    'f1_score': f1_score(y_test, y_pred_final),\n",
    "    'auc_roc': roc_auc_score(y_test, y_pred_proba_final),\n",
    "    'sensitivity': sensitivity,\n",
    "    'specificity': specificity,\n",
    "    'positive_predictive_value': ppv,\n",
    "    'negative_predictive_value': npv\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "for metric, value in results_summary.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"\\n🎉 Breast Cancer Prediction Project Completed Successfully! 🎉\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
